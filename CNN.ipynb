{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import datasets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Overview\n",
    "\n",
    "![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_raw), (x_test, y_test_raw) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3595413c47c846b6911438a24cbb1e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='i', max=30, min=-10), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(i=10) \n",
    "def plot_image(i):\n",
    "    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(y_train_raw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train_raw, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_raw, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,514\n",
      "Trainable params: 61,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 [==============================] - 12s 4ms/step - loss: 0.1851 - accuracy: 0.9436 - val_loss: 0.0936 - val_accuracy: 0.9704\n",
      "Epoch 2/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0542 - accuracy: 0.9827 - val_loss: 0.0536 - val_accuracy: 0.9836\n",
      "Epoch 3/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0375 - accuracy: 0.9884 - val_loss: 0.0544 - val_accuracy: 0.9836\n",
      "Epoch 4/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0292 - accuracy: 0.9910 - val_loss: 0.0505 - val_accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.0412 - val_accuracy: 0.9877\n",
      "Epoch 6/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0716 - val_accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.0441 - val_accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0483 - val_accuracy: 0.9856\n",
      "Epoch 9/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0429 - val_accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "1313/1313 [==============================] - 5s 4ms/step - loss: 0.0085 - accuracy: 0.9968 - val_loss: 0.0514 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18067157850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x=x_train, y=y_train, validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9902999997138977\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - Pesos dos kernels são encontrados durante o treinamento\n",
    "\n",
    "![Pesos ResNet-50](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*NABAG-3XwX0TyTIjOUOPhw.png)\n",
    "\n",
    "Mais informações: https://medium.com/@er_95882/convolution-image-filters-cnns-and-examples-in-python-pytorch-bd3f3ac5df9c\n",
    "\n",
    "![Deep Dream](https://upload.wikimedia.org/wikipedia/commons/d/d0/%22Mona_Lisa%22_with_DeepDream_effect_using_VGG16_network_trained_on_ImageNet.jpg) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
